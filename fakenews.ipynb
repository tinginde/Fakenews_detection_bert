{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fakenews.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinginde/Fakenews_detection_bert/blob/main/fakenews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOe2j9t0IA16"
      },
      "source": [
        "# 資料前處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxefEZFWe3LC",
        "outputId": "e1458da5-4478-46fc-f4b4-e050bc7d7df2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmCTkVTwFX-n"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gddfuTAFFrdg"
      },
      "source": [
        "df_train = pd.read_csv(\"train.csv\", sep=',')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-zb5FwCIzPJ"
      },
      "source": [
        "empty_title = ((df_train['title2_zh'].isnull()) \\\n",
        "               | (df_train['title1_zh'].isnull()) \\\n",
        "               | (df_train['title2_zh'] == '') \\\n",
        "               | (df_train['title2_zh'] == '0'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPZbHWn6Hvu1"
      },
      "source": [
        "df_train = df_train[~empty_title]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYHv0y9kMBJL"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_9JmOAoMxeY"
      },
      "source": [
        "MAX_LENGTH = 30\n",
        "df_train = df_train[~(df_train.title1_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
        "df_train = df_train[~(df_train.title2_zh.apply(lambda x : len(x)) > MAX_LENGTH)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRrcANLQM6VA"
      },
      "source": [
        "SAMPLE_FRAC = 0.01\n",
        "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9527)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O475Szb2NYQ8"
      },
      "source": [
        "df_train = df_train.reset_index()\n",
        "df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
        "df_train.columns = ['text_a', 'text_b', 'label']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3TT2dKzNoBF"
      },
      "source": [
        "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ftC1omRe_t"
      },
      "source": [
        "print(\"訓練樣本數：\", len(df_train))\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp52S3_kRquD",
        "outputId": "76a31e94-7dd9-477b-aefe-868c7a00bb05"
      },
      "source": [
        "df_train.label.value_counts() / len(df_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "unrelated    0.679338\n",
              "agreed       0.294317\n",
              "disagreed    0.026346\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5UI1SmTRRd"
      },
      "source": [
        "df_test = pd.read_csv(\"test.csv\", sep=\",\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1-jLTJ6Sx42"
      },
      "source": [
        "df_test = df_test.loc[:, [\"title1_zh\", \"title2_zh\", \"id\"] ]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD3l72dYUiW8"
      },
      "source": [
        "df_test.columns = [\"text_a\", \"text_b\", \"Id\"]\n",
        "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdV7NLsyUtd2"
      },
      "source": [
        "print(\"預測樣本數：\", len(df_test))\n",
        "df_test.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF0w75xaUvMn",
        "outputId": "83005c02-d64d-483e-d351-e372c8857712"
      },
      "source": [
        "ratio = len(df_test) / len(df_train)\n",
        "print(\"測試集樣本數 / 訓練集樣本數 = {:.1f} 倍\".format(ratio))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "測試集樣本數 / 訓練集樣本數 = 30.2 倍\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWvl_CpaWR0l"
      },
      "source": [
        "# 實作BERT\n",
        "## 要把資料轉換成BERT需要的輸入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxmStICpKFr"
      },
      "source": [
        "## 建立Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAcKJ4VdoQhg"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZGceucdn1MG"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from IPython.display import clear_output\n",
        "\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
        "\n",
        "# 取得此預訓練模型所使用的 tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQlj8Ox-WYP1"
      },
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5-ASPkKWeAF"
      },
      "source": [
        "class FakeNewsDataset(Dataset):\n",
        "  # 讀取處理後的data，並初始化參數\n",
        "  def __init__(self, mode, tokenizer):\n",
        "     assert mode in [\"train\", \"test\"] \n",
        "     self.mode = mode\n",
        "     self.df = pd.read_csv(mode+\".tsv\", sep=\"\\t\").fillna(\"\")\n",
        "     self.len = len(self.df)\n",
        "     self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
        "     self.tokenizer = tokenizer\n",
        "\n",
        "  # 定義一筆回傳訓練與測試的函式\n",
        "  def __getitem__(self, idx):\n",
        "    if self.mode == \"test\":\n",
        "      text_a, text_b = self.df.iloc[idx, :2].values\n",
        "      label_tensor = None\n",
        "    else:\n",
        "      text_a, text_b, label = self.df.iloc[idx, :].values\n",
        "      label_id = self.label_map[label]\n",
        "      label_tensor = torch.tensor(label_id)\n",
        "\n",
        "    # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
        "    word_pieces = [\"[CLS]\"]\n",
        "    tokens_a = self.tokenizer.tokenize(text_a)\n",
        "    word_pieces += tokens_a + [\"[SEP]\"]\n",
        "    len_a = len(word_pieces)\n",
        "\n",
        "    # 建立第二個句子的 BERT tokens\n",
        "    tokens_b = self.tokenizer.tokenize(text_b)\n",
        "    word_pieces += tokens_b + [\"[SEP]\"]\n",
        "    len_b = len(word_pieces) - len_a\n",
        "    # 將整個 token 序列轉換成索引序列\n",
        "    ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "    toeken_tensor = torch.tensor(ids)\n",
        "    # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
        "    segments_tensor = torch.tensor([0]*len_a+ [1]*len_b, dtype = torch.long)\n",
        "\n",
        "    return (toeken_tensor, segments_tensor, label_tensor)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IQH1lTQpYd1"
      },
      "source": [
        "## 建立Dataloader，把data放進來\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuf1GtE-pm-Z"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaI6NcxKpvMT"
      },
      "source": [
        "def create_mini_batch(samples):\n",
        "  tokens_tensors = [s[0] for s in samples]\n",
        "  segments_tensors = [s[1] for s in samples]\n",
        "\n",
        "  # 測試集有 labels\n",
        "  if samples[0][2] is not None:\n",
        "      label_ids = torch.stack([s[2] for s in samples])\n",
        "  else:\n",
        "      label_ids = None\n",
        "\n",
        "  # zero pad 到同一序列長度\n",
        "  tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n",
        "  segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
        "\n",
        "  # 處理attention masks，zero padding 位置不需要attention，所以其他位置設成1\n",
        "  masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
        "  masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
        "\n",
        "  return tokens_tensors, segments_tensors, masks_tensors, label_ids"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK1cNP1bGd3K"
      },
      "source": [
        "初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
        "，利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0i7V4lEsZaA"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX_M03r1GNKs"
      },
      "source": [
        "拿出一個mini-batch內容來看"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqOWT2FHuwce"
      },
      "source": [
        "data = next(iter(trainloader))\n",
        "\n",
        "tokens_tensors, segments_tensors, masks_tensors, label_ids = data"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-woWkPdHVTr"
      },
      "source": [
        "print(f\"\"\"\n",
        "tokens_tensors.shape   = {tokens_tensors.shape} \n",
        "{tokens_tensors}\n",
        "------------------------\n",
        "segments_tensors.shape = {segments_tensors.shape}\n",
        "{segments_tensors}\n",
        "------------------------\n",
        "masks_tensors.shape    = {masks_tensors.shape}\n",
        "{masks_tensors}\n",
        "------------------------\n",
        "label_ids.shape        = {label_ids.shape}\n",
        "{label_ids}\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3LU9q-GvU5B"
      },
      "source": [
        "# 載入BERT可做中文多分類(multi-class)模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg52hf6dvX5d"
      },
      "source": [
        "from transformers import BertForSequenceClassification"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRmQsFs_veEj"
      },
      "source": [
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
        "NUM_LABELS = 3"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR5MDxTYvimB"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwiB_9ZtvsaQ",
        "outputId": "3e2bc639-db2f-4405-be62-b1e99d38bad2"
      },
      "source": [
        "print(\"\"\"\n",
        "name            module\n",
        "----------------------\"\"\")\n",
        "for name, module in model.named_children():\n",
        "    if name == \"bert\":\n",
        "        for n, _ in module.named_children():\n",
        "            print(f\"{name}:{n}\")\n",
        "    else:\n",
        "        print(\"{:15} {}\".format(name, module))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "name            module\n",
            "----------------------\n",
            "bert:embeddings\n",
            "bert:encoder\n",
            "bert:pooler\n",
            "dropout         Dropout(p=0.1, inplace=False)\n",
            "classifier      Linear(in_features=768, out_features=3, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWght4hrSwqo"
      },
      "source": [
        "#預測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPfYYCsiSrM2"
      },
      "source": [
        "def get_predictions(model, dataloader, compute_acc=False):\n",
        "    predictions = None\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for data in dataloader:\n",
        "        if next(model.parameters()).is_cuda:\n",
        "          data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
        "        tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
        "        outputs = model(input_ids=tokens_tensors, \n",
        "                token_type_ids=segments_tensors, \n",
        "                attention_mask=masks_tensors)\n",
        "        logits = outputs[0]\n",
        "        # 返回每一行中最大值的那个元素，且返回其索引(也就是哪一個class是最有可能的答案)\n",
        "        _, pred = torch.max(logits.data, 1)\n",
        "\n",
        "        # 用來計算訓練集的分類準確率\n",
        "        if compute_acc:\n",
        "          labels = data[3]\n",
        "          # size()函数主要是用来统计矩阵元素个数，或矩阵某一维上的元素个数的函数。 \n",
        "          total += labels.size(0)\n",
        "          correct += (pred == labels).sum().item()\n",
        "\n",
        "        # 將當前 batch 記錄下來\n",
        "        if predictions is None:\n",
        "          predictions = pred\n",
        "        # torch.cat: 讓tensor在設定的dim，cat起來\n",
        "        else:\n",
        "          predictions = torch.cat((predictions, pred))\n",
        "    if compute_acc:\n",
        "      acc = correct / total\n",
        "      return predictions, acc\n",
        "    return predictions"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDyCmcNcYboq",
        "outputId": "8577b7f7-1cde-4097-fda0-ae37793fd083"
      },
      "source": [
        "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "model = model.to(device)\n",
        "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
        "print(\"classification acc:\", acc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "classification acc: 0.30033872788859617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU0i_HLOJdIu"
      },
      "source": [
        "# 訓練該下游任務模式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGw8-prNIzp9"
      },
      "source": [
        "# 訓練模式\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8gcKqHJA4N"
      },
      "source": [
        "# 使用 Adam Optim 更新整個分類模型的參數\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKo8lw4sJFEN",
        "outputId": "59d30a2a-6d61-4b8c-a4b0-2a54189bd1aa"
      },
      "source": [
        "EPOCHS = 6  # 幸運數字\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for data in trainloader:\n",
        "\n",
        "    tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
        "    # 將參數梯度歸零\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass\n",
        "    output = model(input_ids=tokens_tensors, \n",
        "            token_type_ids=segments_tensors, \n",
        "            attention_mask=masks_tensors,\n",
        "            labels = labels)\n",
        "    loss = output[0]\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # 紀錄當前 batch loss\n",
        "    running_loss += loss.item()\n",
        "  _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
        "  print('[epoch %d] loss:%.3f ,acc:%.3f' % (epoch+1, running_loss, acc))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 1] loss:30.653 ,acc:0.833\n",
            "[epoch 2] loss:17.536 ,acc:0.883\n",
            "[epoch 3] loss:13.395 ,acc:0.924\n",
            "[epoch 4] loss:9.381 ,acc:0.956\n",
            "[epoch 5] loss:7.012 ,acc:0.912\n",
            "[epoch 6] loss:5.931 ,acc:0.979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbmtQHdTNful"
      },
      "source": [
        "# 用測試集來取得預測結果\n",
        "testset = FakeNewsDataset(\"test\", tokenizer=tokenizer)\n",
        "testloader = DataLoader(testset, batch_size=256, collate_fn=create_mini_batch)\n",
        "predictions = get_predictions(model, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6TvwSlZVrba"
      },
      "source": [
        "# 把預測出的label id 換成 文字\n",
        "index_map = {v:k for k,v in testset.label_map.item()}\n",
        "df = DataFrame({\"Category\": predictions.tolist()})\n",
        "df[\"Category\"] = df.Category.apply(lamda x: index_map[x])\n",
        "df_pred = pd.concat([testset.df.loc[:, [\"ID\"]],df.loc[:, \"Category\"]], axis = 1)\n",
        "df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)\n",
        "df_pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}